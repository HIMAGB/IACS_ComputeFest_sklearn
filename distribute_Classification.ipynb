{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.5.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "# Classification"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "hide": true
      }
    }, 
    {
      "source": [
        "$$\n", 
        "\\renewcommand{\\like}{{\\cal L}}\n", 
        "\\renewcommand{\\loglike}{{\\ell}}\n", 
        "\\renewcommand{\\err}{{\\cal E}}\n", 
        "\\renewcommand{\\dat}{{\\cal D}}\n", 
        "\\renewcommand{\\hyp}{{\\cal H}}\n", 
        "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n", 
        "\\renewcommand{\\x}{{\\mathbf x}}\n", 
        "\\renewcommand{\\v}[1]{{\\mathbf #1}}\n", 
        "$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "variables": {
          "\\cal D": {}, 
          "\\cal E": {}, 
          "\\mathbf #1": {}, 
          "\\mathbf x": {}, 
          "\\cal L": {}, 
          "\\cal H": {}, 
          "\\ell": {}
        }, 
        "hide": true
      }
    }, 
    {
      "source": [
        "We turn our attention to **classification**. Classification tries to predict, which of a small set of classes, a sample in a population belongs to. Mathematically, the aim is to find $y$, a **label** based on knowing a feature vector $\\x$. For instance, consider predicting gender from seeing a person's face, something we do fairly well as humans. To have a machine do this well, we would typically feed the machine a bunch of images of people which have been labelled \"male\" or \"female\" (the training set), and have it learn the gender of the person in the image. Then, given a new photo, the algorithm learned returns us the gender of the person in the photo.\n", 
        "\n", 
        "There are different ways of making classifications. One idea is shown schematically in the image below, where we find a line that divides \"things\" of two different types in a 2-dimensional feature space.\n", 
        "\n", 
        "![Splitting using a single line](images/onelinesplit.png)\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 1, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")\n", 
        "from PIL import Image"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "import warnings\n", 
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "c0=sns.color_palette()[0]\n", 
        "c1=sns.color_palette()[1]\n", 
        "c2=sns.color_palette()[2]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "from matplotlib.colors import ListedColormap\n", 
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n", 
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n", 
        "cm = plt.cm.RdBu\n", 
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n", 
        "\n", 
        "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False, predicted=False):\n", 
        "    h = .02\n", 
        "    X=np.concatenate((Xtr, Xte))\n", 
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", 
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", 
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n", 
        "                         np.linspace(y_min, y_max, 100))\n", 
        "\n", 
        "    #plt.figure(figsize=(10,6))\n", 
        "    if zfunc:\n", 
        "        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n", 
        "        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
        "        Z=zfunc(p0, p1)\n", 
        "    else:\n", 
        "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n", 
        "    ZZ = Z.reshape(xx.shape)\n", 
        "    if mesh:\n", 
        "        plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n", 
        "    if predicted:\n", 
        "        showtr = clf.predict(Xtr)\n", 
        "        showte = clf.predict(Xte)\n", 
        "    else:\n", 
        "        showtr = ytr\n", 
        "        showte = yte\n", 
        "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n", 
        "    # and testing points\n", 
        "    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n", 
        "    ax.set_xlim(xx.min(), xx.max())\n", 
        "    ax.set_ylim(yy.min(), yy.max())\n", 
        "    return ax,xx,yy"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true, 
        "hide": true
      }
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1):\n", 
        "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, cdiscrete=cdiscrete, psize=psize, alpha=alpha, predicted=True) \n", 
        "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
        "    Z = Z.reshape(xx.shape)\n", 
        "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n", 
        "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n", 
        "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14, axes=ax)\n", 
        "    return ax "
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true, 
        "hide": true
      }
    }, 
    {
      "source": [
        "## Using `sklearn`: The heights and weights example"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We'll use a dataset of heights and weights of males and females to hone our understanding of classifiers. We load the data into a dataframe and plot it."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "dflog=pd.read_csv(\"data/01_heights_weights_genders.csv\")\n", 
        "dflog.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Remember that the form of data we will use always is\n", 
        "\n", 
        "![dataform](images/dataform.jpg)\n", 
        "\n", 
        "with the \"response\" as a plain array\n", 
        "\n", 
        "`[1,1,0,0,0,1,0,1,0....]`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 7, 
      "cell_type": "code", 
      "source": [
        "plt.scatter(dflog.Weight, dflog.Height, c=[cm_bright.colors[i] for i in dflog.Gender==\"Male\"], alpha=0.08);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import train_test_split\n", 
        "Xlr, Xtestlr, ylr, ytestlr = train_test_split(dflog[['Height','Weight']].values, (dflog.Gender==\"Male\").values)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "## How to Classify"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Earlier, we used a squared error loss function along with Empirical Risk Minimization (ERM) to carry out regression. The idea there was to calculate this risk on the training set and minimize it. Then the hope was that on the population, or any testing set representative of it, the out-of-sample risk was similar in size to the in-sample training risk, and thus small.\n", 
        "\n", 
        "What might be an appropriate risk for classification? One immediately comes to mind: the fraction of misclassified samples. \n", 
        "\n", 
        "For each sample this is equivalent to choosing the 1-0 loss:\n", 
        "\n", 
        "$$l = \\mathbf{1}_{h \\ne y}.$$\n", 
        "\n", 
        "where $h$ is the classification **decision** we make (for regression we used $l = (h-y)^2$). The symbol $\\mathbf{1}$ means that if $h$ is not equal to the \"true\" value of the point $y$, penalize by 1. Then the risk is:\n", 
        "\n", 
        "$$ R_{\\cal{D}}(h(x)) = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} l = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} \\mathbf{1}_{h \\ne y_i} $$\n", 
        "\n", 
        "Thus if 5 out of 50 samples are misclassified, then the risk is 0.1. This of course means that 90% of the samples are correctly classified. This number is called the **accuracy score** or **utility**:\n", 
        "\n", 
        "$$ U_{\\cal{D}}(h(x))  = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} \\mathbf{1}_{h = y_i} $$\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 64, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import accuracy_score\n", 
        "from sklearn.linear_model import LogisticRegression\n", 
        "clfl=LogisticRegression()\n", 
        "clfl.fit(Xlr, ylr)\n", 
        "ypred=clfl.predict(Xtestlr)\n", 
        "accuracy_score(ypred, ytestlr)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "#from cs109 hw3, 2013\n", 
        "from sklearn.cross_validation import KFold\n", 
        "\n", 
        "def cv_score(clf, x, y, score_func=accuracy_score):\n", 
        "    result = 0\n", 
        "    nfold = 5\n", 
        "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n", 
        "        clf.fit(x[train], y[train]) # fit\n", 
        "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n", 
        "    return result / nfold # average"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 10, 
      "cell_type": "code", 
      "source": [
        "#the grid of parameters to search over\n", 
        "Cs = [0.001, 0.1, 1, 10, 100]\n", 
        "max_score = 0\n", 
        "\n", 
        "for C in Cs:\n", 
        "        clf = LogisticRegression(C=C)\n", 
        "        score = cv_score(clf, Xlr, ylr)\n", 
        "\n", 
        "        if score > max_score:\n", 
        "            max_score = score\n", 
        "            best_C =C\n", 
        "print(max_score, best_C)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 11, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "from sklearn.grid_search import GridSearchCV\n", 
        "clfl2=LogisticRegression()\n", 
        "parameters = {\"C\": [0.0001, 0.001, 0.1, 1, 10, 100]}\n", 
        "fitmodel = GridSearchCV(clfl2, param_grid=parameters, cv=5, scoring=\"accuracy\")\n", 
        "fitmodel.fit(Xlr, ylr)\n", 
        "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "clfl2=LogisticRegression(C=fitmodel.best_params_['C'])\n", 
        "clfl2.fit(Xlr, ylr)\n", 
        "ypred2=clfl2.predict(Xtestlr)\n", 
        "accuracy_score(ypred2, ytestlr)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## The ATM Camera example"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Imagine that you are tasked whith making a smart ATM camera which can distinguish between dollar notes and checks. You want to make sure that dollars are not classified as checks, and that checks are not classified as dollars.\n", 
        "\n", 
        "You are given a set of 87 images of checks and dollars, each of which have been scaled to 322 X 137 pixels, and where each pixel has 3 color channels."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "!cd /tmp;wget https://dl.dropboxusercontent.com/u/75194/imag.pix.npy"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 15, 
      "cell_type": "code", 
      "source": [
        "!cd /tmp;wget https://dl.dropboxusercontent.com/u/75194/imag.lbl.npy"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 16, 
      "cell_type": "code", 
      "source": [
        "!ls -l /tmp/"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 17, 
      "cell_type": "code", 
      "source": [
        "data=np.load(\"/tmp/imag.pix.npy\")\n", 
        "y=np.load(\"/tmp/imag.lbl.npy\")\n", 
        "STANDARD_SIZE = (322, 137)#standardized pixels in image.\n", 
        "data.shape, y.shape"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 18, 
      "cell_type": "code", 
      "source": [
        "def get_image(mat):\n", 
        "    size = STANDARD_SIZE[0]*STANDARD_SIZE[1]*3\n", 
        "    r,g,b = mat[0:size:3], mat[1:size:3],mat[2:size:3]\n", 
        "    rgbArray = np.zeros((STANDARD_SIZE[1],STANDARD_SIZE[0], 3), 'uint8')#3 channels\n", 
        "    rgbArray[..., 0] = r.reshape((STANDARD_SIZE[1], STANDARD_SIZE[0]))\n", 
        "    rgbArray[..., 1] = b.reshape((STANDARD_SIZE[1], STANDARD_SIZE[0]))\n", 
        "    rgbArray[..., 2] = g.reshape((STANDARD_SIZE[1], STANDARD_SIZE[0]))\n", 
        "    return rgbArray\n", 
        "\n", 
        "def display_image(mat):\n", 
        "    with sns.axes_style(\"white\"):\n", 
        "        plt.imshow(get_image(mat))\n", 
        "        plt.xticks([])\n", 
        "        plt.yticks([])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We diaplay some of the images that we have:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 19, 
      "cell_type": "code", 
      "source": [
        "display_image(data[5])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "execution_count": 20, 
      "cell_type": "code", 
      "source": [
        "display_image(data[50])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "What do you think are some of the aspects of these images that will help us distinguish checks from dollars?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The curse of dimensionality: Feature engineering"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The first thing that you notice is that you have many many features: to be precise, $322 x 137 x 3 = 136452$ of them. This is a lot of features! Having too many features can lead to overfitting.\n", 
        "\n", 
        "You have seem this before! Remember when we did the polynomial regression? When we tried to find fits in $\\hyp_1$, there were two features, the constant, and $x$. In $\\hyp_2$, there are 3: the constant, $x$, and $x^2$. When we get to $\\hyp_{20}$, there are 21: the constant, and 20 powers of x. And then we saw how regularization tried to eliminate some of these powers by sending their co-efficients in the polynomial very close to 0, thus reducing the number of features we had.\n", 
        "\n", 
        "Another way to look at this problem is the following: we have 85 data points, but 136452 features; that is, way more features than data points. Thus there is a high chance that a few attributes will correlate with $y$ purely coincidentally!\n", 
        "[^Having lots of images, or \"big-data\" helps in combatting overfitting!]\n", 
        "\n", 
        "We need to do something similar to what happened in the regularized regression here! We will engage in some *a-priori* feature selection that will reduce the dimensionality of the problem. The idea we'll use here is something called **Principal Components Analysis**, or PCA.\n", 
        "\n", 
        "PCA is an unsupervized learning technique. The basic idea behind PCA is to rotate the co-ordinate axes of the feature space. We first find the direction in which the data varies the most. We set up one co-ordinate axes along this direction, which is called the first principal component. We then look for a perpendicular direction in which the data varies the second most. This is the second principal component. The diagram illustrates this process. There are as many principal components as the feature dimension: all we have done is a rotation.\n", 
        "\n", 
        "![pcanim](images/pcanim.gif)\n", 
        "\n", 
        "(diagram taken from http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues which also has nice discussions)\n", 
        "\n", 
        "How does this then achieve feature selection? We decide on a threshold of variation; once the variation in a particular direction falls below a certain number, we get rid of all the co-ordinate axes after that principal component. For example, if the variation falls below 10% after the third axes, and we decide that 10% is an acceptable cutoff, we remove all domensions from the fourth dimension onwards. In other words, we took our higher dimensional problem and projected it onto a 3 dimensional **subspace**.\n", 
        "\n", 
        "We do not have to do this dimensionality reduction unsupervized. Indeed, you will see some supervized dimensionality reduction in the homework.\n", 
        "\n", 
        "These two ideas illustrate one of the most important reasons that learning is even feasible: we believe that **most datasets, in either their unsupervized form $\\{\\v{x\\}}$, or their supervized form $\\{y, \\v{x}\\}$, live on a lower dimensional subspace**. If we can find this subspace, we can then hope to find a methodd which rerpectively separates or fits the data.\n", 
        "\n", 
        "Here we'll continue to focus on PCA. We'll reduce our dimensionality from 136452 to 60. We choose 60 as a large apriori number: we dont know if the variation in the data will have gone below a reasonable threshold by then. Notice that we use `fit_transform` in the `sklearn` API which takes the original 87 rows x 136452 columns dimensional data `data` and transforms it to a 87 x 90 data matrix `X`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 21, 
      "cell_type": "code", 
      "source": [
        "from sklearn.decomposition import PCA\n", 
        "pca = PCA(n_components=60)\n", 
        "X = pca.fit_transform(data)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 22, 
      "cell_type": "code", 
      "source": [
        "pca.explained_variance_ratio_.sum()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The explained variance ratio `pca.explained_variance_ratio_` tells us how much of the variation in the features is explained by these 60 features. When we sum it up over the features, we see that 94% is explained: good enough to go down to a 60 dimensional space from a 136452 dimensional one!\n", 
        "\n", 
        "We can see the individual varainces as we increase the dimensionality:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 23, 
      "cell_type": "code", 
      "source": [
        "pca.explained_variance_ratio_*100"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The first dimension accounts for 35% of the variation, the second 6%, and it goes steadily down from there.\n", 
        "\n", 
        "Let us create a dataframe with these 60 features labelled pc1,pc2...,pc60 and the labels of the sample:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 24, 
      "cell_type": "code", 
      "source": [
        "df = pd.DataFrame({\"y\":y, \"label\":np.where(y==1, \"check\", \"dollar\")})\n", 
        "for i in range(pca.explained_variance_ratio_.shape[0]):\n", 
        "    df[\"pc%i\" % (i+1)] = X[:,i]\n", 
        "df.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets see what these principal components look like:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "def normit(a):\n", 
        "    a=(a - a.min())/(a.max() -a.min())\n", 
        "    a=a*256\n", 
        "    return np.round(a)\n", 
        "def getNC(pc, j):\n", 
        "    size=322*137*3\n", 
        "    r=pc.components_[j][0:size:3]\n", 
        "    g=pc.components_[j][1:size:3]\n", 
        "    b=pc.components_[j][2:size:3]\n", 
        "    r=normit(r)\n", 
        "    g=normit(g)\n", 
        "    b=normit(b)\n", 
        "    return r,g,b\n", 
        "def display_component(pc, j):\n", 
        "    r,g,b = getNC(pc,j)\n", 
        "    rgbArray = np.zeros((137,322,3), 'uint8')\n", 
        "    rgbArray[..., 0] = r.reshape(137,322)\n", 
        "    rgbArray[..., 1] = g.reshape(137,322)\n", 
        "    rgbArray[..., 2] = b.reshape(137,322)\n", 
        "    plt.imshow(rgbArray)\n", 
        "    plt.xticks([])\n", 
        "    plt.yticks([])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "display_component(pca,0)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "display_component(pca,1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We take the first two principal components and immediately notice in the diagram below that they are enough to separate out the checks and the dollars. Indeed the first component itself seems to be mostly enough. We can look at the image of the first component and speculate that the medallion in the middle of the dollars probably contributes to this."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 28, 
      "cell_type": "code", 
      "source": [
        "colors = [c0, c2]\n", 
        "for label, color in zip(df['label'].unique(), colors):\n", 
        "    mask = df['label']==label\n", 
        "    plt.scatter(df[mask]['pc1'], df[mask]['pc2'], c=color, label=label)\n", 
        "plt.legend()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "You might be a bit confused: we needed to use 60 components to explain 94% of the variation in the features, but only 1 or 2 components to separate checks from dollars? This is because PCA is unsupervised: the only variation we are explaining is the variation in the 136452 dimensional feature space. We are not explaining the variation in the $y$ or the label, and it might turn out, as it does in this case, that with the additional information in $y$, the dimensionality needed for classification is much lower.\n", 
        "\n", 
        "We could thus choose just the first few principal components to make our classifier. For the purposes of this lab, since two components can be easily visualized (even though adding some fore features may leads to better separability), we'll go with learning a 2-dimensional classifier in the `pc1` and `pc2` dimensions! [^By the way, there is a problem with pre-doing feature selection before doing cross-validation. Ideally one should be doing the feature selection separately in each fold. The reasons for this is basically that there is a high probability that a feature correlates strongly with $y$ just by chance, if there are so many features. How to do this properly will become clear in the homework.]"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Classifying in a reduced feature space with kNN"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "![m:knn1](images/knn1.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Implicit in the notion of classification, is the idea that samples close to each other in feature-space share a label. kNN is a very simple algorithm to diretly use this idea to do classification. The basic notion is this: if a lot of samples in some area of the feature space belong to one class as compared to the other, we'll label that part of the feature space as \"belonging\" to that class. This process will then classify the feature space into class-based regions. Then, given the point in feature space, we find which region its in and thus its class. \n", 
        "\n", 
        "The way kNN does this is to ask for the k nearest neighbors in the training set of the new sample. To answer this question, one must define a distance in the feature space (Note that this distance is different from the error or risk measures we have seen earlier). This distance is typically defined as the **Euclidean distance**, the sum of the square of the difference of each feature value between any two samples.\n", 
        "\n", 
        "$$D(s_1,s_2) = \\sum_f (x_{f1} - x_{f2})^2.$$\n", 
        "\n", 
        "Once we have a distance measure, we can sort the distances from the current sample. Then we choose the $k$ closest ones in the training set, where $k$ is an odd number (to break ties) like 1,3,5,...19,. We now see how many of these $k$ \"nearest neighbors\" belong to one class or the other, and choose the majority class amongst those neighbors as our sample's class.\n", 
        "\n", 
        "The training process thus simply consists of memorizing the data, perhaps using a database to aid in the fast lookup of the $k$ nearest training set neighbors of any point in feature space. Notice that this process divides feature space into regions of one class or the other, since one can simply ask what the $k$ nearest neighbors in the training set are of any given point in feature space. Also notice that since classification happens via a majority \"voting\" scheme, we also know the probability that a point in feature space belongs to a class, as estimated by the fraction of $k$ nearest neighbors to that point in the desired class.\n", 
        "\n", 
        "Thanks to `sklearn`'s simple api, the classifier is really simple to write:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n", 
        "from sklearn.cross_validation import train_test_split\n", 
        "ys=df['y'].astype(int).values\n", 
        "subdf=df[['pc1','pc2']]\n", 
        "subdfstd=(subdf - subdf.mean())/subdf.std()\n", 
        "Xs=subdfstd.values\n", 
        "def classify(X,y, nbrs, plotit=True, train_size=0.6):\n", 
        "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size)\n", 
        "    clf= KNeighborsClassifier(nbrs)\n", 
        "    clf=clf.fit(Xtrain, ytrain)\n", 
        "    #in sklearn accuracy can be found by using \"score\". It predicts and then gets the accuracy\n", 
        "    training_accuracy = clf.score(Xtrain, ytrain)\n", 
        "    test_accuracy = clf.score(Xtest, ytest)\n", 
        "    Xall=np.concatenate((Xtrain, Xtest))\n", 
        "    if plotit:\n", 
        "        print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n", 
        "        print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n", 
        "        plt.figure()\n", 
        "        ax=plt.gca()\n", 
        "        points_plot(ax, Xtrain, Xtest, ytrain, ytest, clf, alpha=0.3, psize=20)\n", 
        "    return nbrs, training_accuracy, test_accuracy"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets see what happens when we choose $k=1$. On the training set, the 1NN classifier memorizes the training data. It will predict perfectly on the training set, and wont do too badly on the test set, especially deep in the regions of feature space where one or the other class dominates. This is because evem one neighbor might be enough in those regions. However, the same classifier will do badly near the clasification boundaries on the test set, because you will need more than one neighbor to decide with any certainty of the class.\n", 
        "\n", 
        "The result of this is, as you might expect, the regions of feature space classfied one way or the other (blue is check, red is dollar) are quite jagged and mottled. Since we are choosing just one neighbor, we fit to the noise in the region rather than the trend. We are overfitting."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "classify(Xs,ys,1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "If we choose too large a number for $k$, such as 50, we are wandering too far from our original sample,and thus we average over a large amount of the feature space. This leads to a very biased classification, depending on where our sample is, but extending far out from there. Our classification may even cover the entire feature space, then giving us the majority class.\n", 
        "\n", 
        "In terms of probabilities, such an underfit case gives us the **base rate** classifier. Imagine $k=N$. Then the probability is just the fraction of training set examples in a given class. Say this number for the blue class is 0.4 (that is, whe have uneven class memberships in the training set). Now, on any random test set, if we use the classifier which says \"classify all as red\", we will be correct, on average, 60% of the time if the test set and training sets are representative of the population of samples. Any classifier we create must do a better job than this!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "classify(Xs,ys,50)#run this a few times"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "![m:knn2](images/knn2.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "These notions of overfitting and underfitting are illustrated in the image above: the small circles represent small $k$ neighborhoods while the large circle indicates a large $k$ neighborhood. The lower left part of that circle would likely be classified red even though it would seem to be visually more on the blue side of things.\n", 
        "\n", 
        "The circle of in-between size illustrates what a reasonable $k$ might do. Also notice that kNN will be quite stable far away from the classification bondary, but is likely to be more jagged near the classification boundary. Having a reasonable number for $k$ will \"smooth\" the jaged edges out.\n", 
        "\n", 
        "We thus once again need to find the balance between the high bias (large $k$) case and the high variance (low $k$) case, and once again we turn to our error vs complexity curve to find the appropriate $k$,"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Error against complexity (k), and cross-validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "fits={}\n", 
        "for k in np.arange(1,45,1):\n", 
        "    fits[k]=[]\n", 
        "    for i in range(200):\n", 
        "        fits[k].append(classify(Xs, ys,k, False))\n", 
        "nbrs=np.arange(1,45,1)\n", 
        "fmeanstr = np.array([1.-np.mean([t[1] for t in fits[e]]) for e in nbrs])\n", 
        "fmeanste = np.array([1.-np.mean([t[2] for t in fits[e]]) for e in nbrs])\n", 
        "fstdsstr = np.array([np.std([t[1] for t in fits[e]]) for e in nbrs])\n", 
        "fstdsste = np.array([np.std([t[2] for t in fits[e]]) for e in nbrs])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 33, 
      "cell_type": "code", 
      "source": [
        "plt.gca().invert_xaxis()\n", 
        "plt.plot(nbrs, fmeanstr, color=c0, label=\"training\");\n", 
        "plt.fill_between(nbrs, fmeanstr - fstdsstr, fmeanstr+fstdsstr, color=c0, alpha=0.3)\n", 
        "plt.plot(nbrs, fmeanste, color=c1, label=\"testing\");\n", 
        "plt.fill_between(nbrs, fmeanste - fstdsste, fmeanste+fstdsste, color=c1, alpha=0.5)\n", 
        "\n", 
        "plt.legend();"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Once again, as before, we plot the test error and training errors against the number of neighbors $k$ . Here $k$ serves as a complexity parameter, with small $k$ being more \"wiggly\" in the classification of neighborhoods and large $k$ oversmoothing the classification. Notice that we plot $k$ reversed on the x-axis so as to go from lower complexity to higher complexity. As expected, the training error drops with complexity, but the test error starts going back up. There is a large range of $k$ from 25 to 5, in which the fit is as good as it gets!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Setting up some code"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Lets make a small diversion, though, and set some code up for classification using cross-validation so that we can easily run classification models in scikit-learn. We first set up a function `cv_optimize` which takes a classifier `clf`, a grid of hyperparameters (such as a complexity parameter or regularization parameter as in the last ) implemented as a dictionary `parameters`, a training set (as a samples x features array) `Xtrain`, and a set of labels `ytrain`. The code takes the traning set, splits it into `n_folds` parts, sets up `n_folds` folds, and carries out a cross-validation by splitting the training set into a training and validation section for each foldfor us. It prints the best value of the parameters, and retuens the best classifier to us."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 34, 
      "cell_type": "code", 
      "source": [
        "def cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=5):\n", 
        "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n", 
        "    gs.fit(Xtrain, ytrain)\n", 
        "    print(\"BEST PARAMS\", gs.best_params_)\n", 
        "    best = gs.best_estimator_\n", 
        "    return best"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We then use this best classifier to fit the entire training set. This is done inside the `do_classify` function which takes a dataframe `indf` as input. It takes the columns in the list `featurenames` as the features used to train the classifier. The column `targetname` sets the target. The classification is done by setting those samples for which `targetname` has value `target1val` to the value 1, and all others to 0. We split the dataframe into 80% training and 20% testing by default, standardizing the dataset if desired. (Standardizing a data set involves scaling the data so that it has 0 mean and is described in units of its standard deviation. We then train the model on the training set using cross-validation. Having obtained the best classifier using `cv_optimize`, we retrain on the entire training set and calculate the training and testing accuracy, which we print. We return the split data and the trained classifier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 35, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import train_test_split\n", 
        "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, standardize=False, train_size=0.8):\n", 
        "    subdf=indf[featurenames]\n", 
        "    if standardize:\n", 
        "        subdfstd=(subdf - subdf.mean())/subdf.std()\n", 
        "    else:\n", 
        "        subdfstd=subdf\n", 
        "    X=subdfstd.values\n", 
        "    y=(indf[targetname].values==target1val)*1\n", 
        "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size)\n", 
        "    print(\"PARAMS\", parameters)\n", 
        "    clf = cv_optimize(clf, parameters, Xtrain, ytrain)\n", 
        "    clf=clf.fit(Xtrain, ytrain)\n", 
        "    training_accuracy = clf.score(Xtrain, ytrain)\n", 
        "    test_accuracy = clf.score(Xtest, ytest)\n", 
        "    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n", 
        "    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n", 
        "    return clf, Xtrain, ytrain, Xtest, ytest"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true, 
        "hide": true
      }
    }, 
    {
      "source": [
        "### As before, cross-validation"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Lets repeat what we have been doing so far and carry out a cross-validation. We're of-course now training on an even smaller set, so our results will be a bit different from the diagram above. We plot the results in the diagram below. The results are fairly stable and correspond to our intuition that the first principal component basically separates the data."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 36, 
      "cell_type": "code", 
      "source": [
        "bestcv, Xtrain, ytrain, Xtest, ytest = do_classify(KNeighborsClassifier(), {\"n_neighbors\": list(range(1,40,2))}, df, ['pc1','pc2'], 'label', 'check' )"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 37, 
      "cell_type": "code", 
      "source": [
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "points_plot(ax, Xtrain, Xtest, ytrain, ytest, bestcv, alpha=0.5, psize=20);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "We can plot the probability contours as well: the probability is easily obtained by just counting the fraction of neighbors that are blue or red."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 38, 
      "cell_type": "code", 
      "source": [
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "points_plot_prob(ax, Xtrain, Xtest, ytrain, ytest, bestcv, alpha=0.5, psize=20);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "\n", 
        "You have encountered nearest neighbor classifiers on the web before. They are components in the systems companies like Amazon, Netflix, and Yelp use when they tell you: if you liked movie A and movie B, you might like movie C. Later in the homework we shall use the Yelp data to create a restaurant recommender using kNN.\n", 
        "\n", 
        "A critical part of writing such classifiers is finding the right features, since irrelevant features simply add additional noise into the distances in feature space. This is a particular form of the curse of dimensionality. Another way to deal with this problem is to assign greater weights to distances in certain dimensions, such as the \"violentness\" of a movie for example. Finally one may modify the voting mechanism or probability estimation for classifying from a simple majority class mechanism to one in which nearer examples in the k nearest examples are given more weight.\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Logistic Regression"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Remember the 1-0 loss we talked about earlier? It turns out that minimizing the 1-0 loss with respect to some parameters of our model is very hard to do, for technical reasons of the 1-0 loss not being a convex loss. Thus it makes sense to look for alternate approaches. One approach that we know about already is linear regression.\n", 
        "\n", 
        "We could approach classification as linear regression, there the class, 0 or 1, is the target variable $y$. But this ignores the fact that our output $y$ is discrete valued, and futhermore, the $y$ predicted by linear regression will in general take on values less than 0 and greater than 1. Thus this does not seem like a very good idea.\n", 
        "\n", 
        "But what if we could change the form of our hypotheses $h(x)$ instead?\n", 
        "\n", 
        "The idea behind logistic regression is very simple. We want to draw a line in feature space that divides the '1' samples from the '0' samples, just like in the diagram above. In other words, we wish to find the \"regression\" line which divides the samples. Now, a line has the form $w_1 x_1 + w_2 x_2 + w_0 = 0$ in 2-dimensions. On one side of this line we have \n", 
        "\n", 
        "$$w_1 x_1 + w_2 x_2 + w_0 \\ge 0,$$\n", 
        "\n", 
        "and on the other side we have \n", 
        "\n", 
        "$$w_1 x_1 + w_2 x_2 + w_0 < 0.$$ \n", 
        "\n", 
        "Our classification rule then becomes:\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        "y = 1 &if& \\v{w}\\cdot\\v{x} \\ge 0\\\\\n", 
        "y = 0 &if& \\v{w}\\cdot\\v{x} < 0\n", 
        "\\end{eqnarray*}\n", 
        "\n", 
        "where $\\v{x}$ is the vector $\\{1,x_1, x_2,...,x_n\\}$ where we have also generalized to more than 2 features.\n", 
        "\n", 
        "What hypotheses $h$ can we use to achieve this? One way to do so is to use the **sigmoid** function:\n", 
        "\n", 
        "$$h(z) = \\frac{1}{1 + e^{-z}}.$$\n", 
        "\n", 
        "Notice that at $z=0$ this function has the value 0.5. If $z > 0$, $h > 0.5$ and as $z \\to \\infty$, $h \\to 1$. If $z < 0$, $h < 0.5$ and as $z \\to -\\infty$, $h \\to 0$. As long as we identify any value of $y > 0.5$ as 1, and any $y < 0.5$ as 0, we can achieve what we wished above.\n", 
        "\n", 
        "This function is plotted below:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 39, 
      "cell_type": "code", 
      "source": [
        "h = lambda z: 1./(1+np.exp(-z))\n", 
        "zs=np.arange(-5,5,0.1)\n", 
        "plt.plot(zs, h(zs), alpha=0.5);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So we then come up with our rule by identifying:\n", 
        "\n", 
        "$$z = \\v{w}\\cdot\\v{x}.$$\n", 
        "\n", 
        "Then $h(\\v{w}\\cdot\\v{x}) \\ge 0.5$ if $\\v{w}\\cdot\\v{x} \\ge 0$ and $h(\\v{w}\\cdot\\v{x}) \\lt 0.5$ if $\\v{w}\\cdot\\v{x} \\lt 0$, and:\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        "y = 1 &if& h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\\\\n", 
        "y = 0 &if& h(\\v{w}\\cdot\\v{x}) \\lt 0.5.\n", 
        "\\end{eqnarray*}\n", 
        "\n", 
        "We will show soon that this identification can be achieved by minimizing a loss in the ERM framework called the **log loss** :\n", 
        "\n", 
        "$$ R_{\\cal{D}}(\\v{w}) = - \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )$$\n", 
        "\n", 
        "More generally we add a regularization term (as in the ridge regression):\n", 
        "\n", 
        "$$ R_{\\cal{D}}(\\v{w}) = - \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right ) + \\frac{1}{C} \\v{w}\\cdot\\v{w},$$\n", 
        "\n", 
        "where $C$ is the regularization strength (corresponding to $1/\\alpha$ from the Ridge case), and smaller values of $C$ mean stronger regularization. As before, the regularization tries to prevent features from having terribly high weights, thus implementing a form of feature selection. \n", 
        "\n", 
        "How did we come up with this loss? We'll come back to that, but let us see how logistic regression works out. \n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 40, 
      "cell_type": "code", 
      "source": [
        "dflog.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 41, 
      "cell_type": "code", 
      "source": [
        "clf_l, Xtrain_l, ytrain_l, Xtest_l, ytest_l  = do_classify(LogisticRegression(), {\"C\": [0.01, 0.1, 1, 10, 100]}, dflog, ['Weight', 'Height'], 'Gender','Male')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "points_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "In the figure here showing the results of the logistic regression, we plot the actual labels of both the training(circles) and test(squares) samples. The 0's (females) are plotted in red, the 1's (males) in blue. We also show the classification boundary, a line (to the resolution of a grid square). Every sample on the red background side of the line will be classified female, and every sample on the blue side, male. Notice that most of the samples are classified well, but there are misclassified people on both sides, as evidenced by leakage of dots or squares of one color ontothe side of the other color. Both test and traing accuracy are about 92%."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The probabilistic interpretaion"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Remember we said earlier that if $h > 0.5$ we ought to identify the sample with $y=1$? One way of thinking about this is to identify $h(\\v{w}\\cdot\\v{x})$ with the probability that the sample is a '1' ($y=1$). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a '1' is $\\ge 0.5$.\n", 
        "\n", 
        "So suppose we say then that the probability of $y=1$ for a given $\\v{x}$ is given by $h(\\v{w}\\cdot\\v{x})$?\n", 
        "\n", 
        "Then, the conditional probabilities of $y=1$ or $y=0$ given a particular sample's features $\\v{x}$ are:\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        "P(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\n", 
        "P(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n", 
        "\\end{eqnarray*}\n", 
        "\n", 
        "These two can be written together as\n", 
        "\n", 
        "$$P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} $$\n", 
        "\n", 
        "Then multiplying over the samples we get the probability of the training $y$ given $\\v{w}$ and the $\\v{x}$:\n", 
        "\n", 
        "$$P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}$$\n", 
        "\n", 
        "Why use probabilities? Earlier, we talked about how the regression function $f(x)$ never gives us the $y$ exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently. \n", 
        "\n", 
        "We said that another way to think about a noisy $y$ is to imagine that our data $\\dat$ was generated from  a joint probability distribution $P(x,y)$. Thus we need to model $y$ at a given $x$, written as $P(y|x)$, and since $P(x)$ is also a probability distribution, we have:\n", 
        "\n", 
        "$$P(x,y) = P(y | x) P(x) ,$$\n", 
        "\n", 
        "and can obtain our joint probability ($P(x, y))$.\n", 
        "\n", 
        "Indeed its important to realize that a particular training set can be thought of as a draw from some \"true\" probability distribution (just as we did when showing the hairy variance diagram). If for example the probability of classifying a test sample as a '0' was 0.1, and it turns out that the test sample was a '0', it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a '0'! But, of-course its more unlikely than its likely, and having good probabilities means that we'll be likely right most of the time, which is what we want to achieve in classification. And furthermore, we can quantify this accuracy.\n", 
        "\n", 
        "Thus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a '1'. There are business reasons for this too. Consider the example of customer \"churn\": you are a cell-phone company and want to know, based on some of my purchasing habit and characteristic \"features\" if I am a likely defector. If so, you'll offer me an incentive not to defect. In this scenario, you might want to know which customers are most likely to defect, or even more precisely, which are most likely to respond to incentives. Based on these probabilities, you could then spend a finite marketing budget wisely."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Maximizing the probability of the training set."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now if we maximize $$P(y|\\v{x},\\v{w})$$, we will maximize the chance that each point is classified correctly, which is what we want to do. While this is not exactly the same thing as maximizing the 1-0 training risk, it is a principled way of obtaining the highest probability classification. This process is called **maximum likelihood** estimation since we are maximising the **likelihood of the training data y**, \n", 
        "\n", 
        "$$\\like = P(y|\\v{x},\\v{w}).$$ \n", 
        "\n", 
        "Maximum likelihood is one of the corenerstone methods in statistics, and is used to estimate probabilities of data. \n", 
        "\n", 
        "We can equivalently maximize \n", 
        "\n", 
        "$$\\loglike = log(P(y|\\v{x},\\v{w}))$$ \n", 
        "\n", 
        "since the natural logarithm $log$ is a monotonic function. This is known as maximizing the **log-likelihood**. Thus we can equivalently *minimize* a risk that is the negative of  $log(P(y|\\v{x},\\v{w}))$:\n", 
        "\n", 
        "$$R_{\\cal{D}}(h(x)) = -\\loglike = -log \\like = - log(P(y|\\v{x},\\v{w})).$$\n", 
        "\n", 
        "\n", 
        "Thus\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        "R_{\\cal{D}}(h(x)) &=& -log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n", 
        "                  &=& -\\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n", 
        "                  &=& -\\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n", 
        "                  &=& - \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n", 
        "\\end{eqnarray*}\n", 
        "                  \n", 
        "This is exactly the risk we had above, leaving out the regularization term (which we shall return to later) and was the reason we chose it over the 1-0 risk. \n", 
        "\n", 
        "Notice that this little process we carried out above tells us something very interesting: **Probabilistic estimation using maximum likelihood is equivalent to Empiricial Risk Minimization using the negative log-likelihood**, since all we did was to minimize the negative log-likelihood over the training samples.\n", 
        "\n", 
        "`sklearn` will return the probabilities for our samples, or for that matter, for any input vector set $\\{\\v{x}_i\\}$, i.e. $P(y_i | \\v{x}_i, \\v{w})$:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 43, 
      "cell_type": "code", 
      "source": [
        "clf_l.predict_proba(Xtest_l)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 44, 
      "cell_type": "code", 
      "source": [
        "plt.figure()\n", 
        "ax=plt.gca()\n", 
        "points_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Notice that lines of equal probability, as might be expected are stright lines. What the classifier does is very intuitive: if the probability is greater than 0.5, it classifies the sample as type '1' (male), otherwise it classifies the sample to be class '0'. Thus in the diagram above, where we have plotted predicted values rather than actual labels of samples, there is a clear demarcation at the 0.5 probability line.\n", 
        "\n", 
        "This notion of trying to obtain the line or boundary of demarcation is what is called a **discriminative** classifier. The algorithm tries to find a decision boundary that separates the males from the females. To classify a new sample as male or female, it checks on which side of the decision boundary the sample falls, and makes a prediction. In other words we are asking, given $\\v{x}$, what is the probability of a given $y$, or, what is the likelihood $P(y|\\v{x},\\v{w})$?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## The multiple risks in classification"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "With all this talk about probabilities, maximizing likelihood, and negative log-likelihoods as equivalent risks, you might be wondering if we have bitten the shark. Our original premise was a very simple empirical risk minimization (ERM). We just took all the points in the training or test set, and summed up the risk over them, dividing by the number of points in the set. Here we seem to be talking about solving a much harder problem, the estimation of probabilities.\n", 
        "\n", 
        "We have some flavor of this simple minimization in this discriminative approach, since we do use ERM on the training set with the negatice log-likelihood as the risk. But we appear to have lost the direct notion of function estimation that we had proposed in the regression case. Instead we are indulging in probability estimation with a notion that a probability > 0.5 (in the two class case) is the key to making a classification.\n", 
        "\n", 
        "Probability comes in because of stochastic noise and our incomplete knowledge about the world. Minimizing risk is an idea which works (as we saw in the noiseless regression case) even without any stochastic noise. Where do these two ideas meet? \n", 
        "\n", 
        "We minimized a risk (maximised likelihood) to estimate probability in both Logistic Regression (log-loss) and LDA above. This risk is called the **Estimation Risk**. But there is a second risk lurking here: one we call the **Decision Risk**. The final result we desire is not a list of probabilities; but rather a **decision**-making process on how to classify a given sample. The decision risk converts these probabilities into classifications. For example, we make an intuitive decision to classify a sample as a '1' if the probability of the sample being a '1' is greater than 0.5. It turns out that this intution actually uses the 1-0 risk we introduced earlier as a decision risk, as we shall show in the next . \n", 
        "\n", 
        "Why is separating the decision making process from the probability estimation process important? Consider as another example the case of predicting cancer based on features obtained from some tests, mammograms, etc. We dont want to predict a no-cancer for someone who has it (if we consider cancer as class '1', the positive class, this would be a false negative). This might be a death sentence (not to mention the liability lawsuit). The opposite, predicting cancer for someone who dosent have it (a false positive) is not as bad, obviously. Now the threshold probability for choosing '1' ought to be lower than 0.5, as we ought to be predicting \"cancer\" at even the whiff of it. This is **asymmetric risk**: we'll show in the next  that we need to modify the 1-0 risk to account for this *decision making asymmetry*.\n", 
        "\n", 
        "Thus we need to consider *two risks in learning*, one to *estimate probabilities*, and one to *make decisions*."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Evaluation and The Confusion Matrix"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We have written two classifiers. A classifier will get some samples right, and some wrong. Generally we see which ones it gets right and which ones it gets wrong on the test set. There,\n", 
        "\n", 
        "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP)\n", 
        "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP)\n", 
        "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN)\n", 
        "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN)\n", 
        "\n", 
        "A classifier produces a confusion matrix from these which lookslike this:\n", 
        "\n", 
        "![hwimages](./images/confusionmatrix.png)\n", 
        "\n", 
        "\n", 
        "IMPORTANT NOTE: In sklearn, to obtain the confusion matrix in the form above, always have the observed `y` first, i.e.: use as `confusion_matrix(y_true, y_pred)`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 46, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import confusion_matrix\n", 
        "confusion_matrix(ytest_l, clf_l.predict(Xtest_l))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Given these definitions, we typically calculate a few metrics for our classifier. First, the **True Positive Rate**:\n", 
        "\n", 
        "$$TPR = Recall = \\frac{TP}{OP} = \\frac{TP}{TP+FN},$$\n", 
        "\n", 
        "also called the Hit Rate: the fraction of observed positives (1s) the classifier gets right, or how many true positives were recalled. Maximizing the recall towards 1 means keeping down the false negative rate. In a classifier try to find cancer patients, this is the number we want to maximize.\n", 
        "\n", 
        "The **False Positive Rate** is defined as\n", 
        "\n", 
        "$$FPR = \\frac{FP}{ON} = \\frac{FP}{FP+TN},$$\n", 
        "\n", 
        "also called the False Alarm Rate, the fraction of observed negatives (0s) the classifier gets wrong. In general, you want this number to be low. Instead, you might want to maximize the\n", 
        "**Precision**,which tells you how many of the predicted positive(1) hits were truly positive\n", 
        "\n", 
        "$$Precision = \\frac{TP}{PP} = \\frac{TP}{TP+FP}.$$\n", 
        "\n", 
        "Finally the **F1** score gives us the Harmonic Score of Precision and Recall. Many analysts will try and find a classifier that maximizes this score, since it tries to minimize both false positives and false negatives simultaneously, and is thus a bit more precise in what it is trying to do than the accuracy.\n", 
        "\n", 
        "$$F1 =  \\frac{2*Recall*Precision}{Recall + Precision}$$\n", 
        "\n", 
        "However, in a case like that of a cancer classifier, we will wish to minimize false nagatives at the expense of false positives: it is ok to send perfectly healthy patients for cancer folloup if that is the price we must pay for not missing any sick ones.\n", 
        "\n", 
        "`scikit-learn` helpfully gives us a classification report with all these numbers"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 47, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import classification_report\n", 
        "print(classification_report(ytest_l, clf_l.predict(Xtest_l)))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## ROC Curve\n", 
        "\n", 
        "The images in this section are from Provost, Foster; Fawcett, Tom (2013-07-27). Data Science for Business: What you need to know about data mining and data-analytic thinking  O'Reilly Media. Great book!\n", 
        "\n", 
        "We can check on the thresholds we talked about in the previous section and compare our classifiers to each other and the baseline models using the ROC curves you learned about in class. \n", 
        "\n", 
        "Remember that ROC curves are actually a set of classifiers, in which we move the threshold for classifying a sample as positive from 1 to 0. Each point on a ROC curve is a separate classifier obtained by considering a different threshold. (In the standard scenario, where we used the  classifier accuracy, this threshold is implicitly set at 0.5, and we have only one classifier).\n", 
        "\n", 
        "![m:roc curve](images/roc-curve.png)\n", 
        "\n", 
        "The way ROC curves are calulated is this. We start with a large threshold, something like 0.99 or so. This means that only samples with a probability of being positive higher than that threshold are classified as positive. That is the really really really positive ones! The idea then is to decrease this threshold, such that more and more samples get classified as positive.\n", 
        "\n", 
        "![howto roc](images/howtoroc.png)\n", 
        "\n", 
        "The practical way to do this is to order the samples by probability of being positive, or in the case of the SVM, by the `decision_function` or distance from the separating hyperplane. Then consider the sample with the highest score or highest probability of being positive. At first, only this sample is positive. Then, we take the sample with the next highest score, and call it positive. As we go down the list, we go down a threshold in score or probability. \n", 
        "\n", 
        "Now, for each such situation: only 1 positive, now 2 positive,....you can imagine a different classifier with a different confusion matrix. It will have its own false positives, tre positives, etc. Its actually the same original classifier, but with a different threshold each time.\n", 
        "\n", 
        "As we keep going down the list, decreasing the threshold, more and more samples become positive, and at first, the true positives rise faster than the false positives. Once past a certain point, false positives increase faster than true positives. Now, if you want a balanced classifier, you look at this turn-around point...the northwest corner, so to speak. But if you want a classifier which penalizes false positives and false negatives differently, the point you want is different.\n", 
        "\n", 
        "Here is the confusion matrix again:\n", 
        "\n", 
        "![hwimages](./images/confusionmatrix.png)\n", 
        "\n", 
        "\n", 
        "To make a ROC curve you plot the True Positive Rate, \n", 
        "\n", 
        "$$TPR=\\frac{TP}{OP}$$\n", 
        "\n", 
        "against the False Positive Rate,\n", 
        "\n", 
        "$$FPR=\\frac{FP}{ON}$$\n", 
        "\n", 
        "as you go through this process of going down the list of samples. ROC curves are useful because they calculate one classifier per threshold and show you where you are in TPR/FPR space without making any assumptions about the utility matrix or which threshold is appropriate.\n", 
        "\n", 
        "Notice that the ROC curve has a very interesting property: if you look at the confusion matrix above, TPR is only calculated from the observed \"1\" row while FPR is calculated from the observed '0' row. This means that the ROC curve is independent of the class balance/imbalance on the test set, and thus works for all ratios of positive to negative samples. The balance picks a point on the curve, as you can read below.\n", 
        "\n", 
        "A rote reading of the ROC curve (go to the \"northwest\" corner) is a bad idea: you must fold in the curve with any assumptions you are making about costs. More on this in the next lab. Still, on the whole, a curve with a greater AUC (area under curve), or further away from the line of randomness, will give us a rough idea of what might be a better classifier.\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 48, 
      "cell_type": "code", 
      "source": [
        "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n", 
        "    initial=False\n", 
        "    if not ax:\n", 
        "        ax=plt.gca()\n", 
        "        initial=True\n", 
        "    if proba:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n", 
        "    else:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n", 
        "    roc_auc = auc(fpr, tpr)\n", 
        "    if skip:\n", 
        "        l=fpr.shape[0]\n", 
        "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n", 
        "    else:\n", 
        "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n", 
        "    label_kwargs = {}\n", 
        "    label_kwargs['bbox'] = dict(\n", 
        "        boxstyle='round,pad=0.3', alpha=0.2,\n", 
        "    )\n", 
        "    for k in range(0, fpr.shape[0],labe):\n", 
        "        #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n", 
        "        threshold = str(np.round(thresholds[k], 2))\n", 
        "        ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n", 
        "    if initial:\n", 
        "        ax.plot([0, 1], [0, 1], 'k--')\n", 
        "        ax.set_xlim([0.0, 1.0])\n", 
        "        ax.set_ylim([0.0, 1.05])\n", 
        "        ax.set_xlabel('False Positive Rate')\n", 
        "        ax.set_ylabel('True Positive Rate')\n", 
        "        ax.set_title('ROC')\n", 
        "    ax.legend(loc=\"lower right\")\n", 
        "    return ax"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 49, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import roc_curve, auc\n", 
        "ax=make_roc(\"logistic\", clf_l, ytest_l, Xtest_l, labe=100)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### But is accuracy the correct metric?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 50, 
      "cell_type": "code", 
      "source": [
        "dfchurn=pd.read_csv(\"data/churn.csv\")\n", 
        "dfchurn.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 51, 
      "cell_type": "code", 
      "source": [
        "ychurn = np.where(dfchurn['Churn?'] == 'True.',1,0)\n", 
        "ychurn.mean()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "First notice that our data set is very highly asymmetric, with positives, or people who churned, only making up 14-15% of the samples. TODO\n", 
        "\n", 
        "Consider two classifiers, A and B, as in the image below. Suppose they were trained on a balanced set. Let A make its mistakes only through false positives: non-churners(n) predicted to churn(Y), while B makes its mistake only through false negatives, churners(p), predicted not to churn(N). Now consider what this looks like on an unbalanced set, where the ps (churners) are much less than the ns (non-churners). It would seem that B makes far fewer misclassifications based on accuracy than A, and would thus be a better classifier.\n", 
        "\n", 
        "![m:abmodeldiag](./images/abmodeldiag.png)\n", 
        "\n", 
        "However, is B reaslly the best classifier for us? False negatives are people who churn, but we predicted them not to churn.These are very costly for us. So for us. classifier A might be better, even though, on the unbalanced set, it is way less accurate!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Whenever you are comparing classifiers you should always establish a baseline, one way or the other.  In our churn dataset there are two obvious baselines: assume every customer wont churn, and assume all customers will churn.\n", 
        "\n", 
        "The former baseline, will on our dataset, straight away give you a 85.5% accuracy. If you are planning on using accuracy, any classifier you write ought to beat this. The other baseline, from an accuracy perspective is less interesting: it would only have a 14.5% correct rate."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 52, 
      "cell_type": "code", 
      "source": [
        "dfchurn.columns"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 53, 
      "cell_type": "code", 
      "source": [
        "dfchurn[\"Int'l Plan\"] = dfchurn[\"Int'l Plan\"]=='yes'\n", 
        "dfchurn[\"VMail Plan\"] = dfchurn[\"VMail Plan\"]=='yes'\n", 
        "colswewant=[ u'Account Length', u\"Int'l Plan\", u'VMail Plan', u'VMail Message', u'Day Mins', u'Day Calls', u'Day Charge', u'Eve Mins', u'Eve Calls', u'Eve Charge', u'Night Mins', u'Night Calls', u'Night Charge', u'Intl Mins', u'Intl Calls', u'Intl Charge', u'CustServ Calls']"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 54, 
      "cell_type": "code", 
      "source": [
        "clfl = LogisticRegression()\n", 
        "parameters = {\"C\": [1e-5, 1e-3, 1e-1, 1,10,100,1000, 10000]}\n", 
        "clfl, Xtrain, ytrain, Xtest, ytest=do_classify(clfl, parameters, dfchurn, colswewant,'Churn?', \"True.\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 55, 
      "cell_type": "code", 
      "source": [
        "clfknn = KNeighborsClassifier()\n", 
        "parameters = {\"n_neighbors\": list(range(1,40,1))}\n", 
        "clfknn, Xtrain, ytrain, Xtest, ytest=do_classify(clfknn, parameters, dfchurn, colswewant,'Churn?', \"True.\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 56, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## For Later: Costs \n", 
        "But as we have seen, on such asymmetric data sets, accuracy is just not a good metric. So what should we use?\n", 
        "\n", 
        "**A metric ought to hew to the business function that the classifier is intended for**.\n", 
        "\n", 
        "But to do this we need to understand the business situation. To do this, we write a **utility**, or, equivalently, **cost** matrix associated with the 4 scenarios that the confusion matrix talks about. \n", 
        "\n", 
        "![cost matrix](images/costmatrix.png)\n", 
        "\n", 
        "Remember that +ives or 1s are churners, and -ives or 0s are the ones that dont churn. \n", 
        "\n", 
        "Lets assume we make an offer with an administrative cost of \\$3 and an offer cost of \\$100, an incentive for the customer to stay with us. If a customer leaves us, we lose the customer lifetime value, which is some kind of measure of the lost profit from that customer. Lets assume this is the average number of months a customer stays with the telecom times the net revenue from the customer per month. We'll assume 3 years and \\$30/month margin per user lost, for roughly a $1000 loss."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 57, 
      "cell_type": "code", 
      "source": [
        "admin_cost=3\n", 
        "offer_cost=100\n", 
        "clv=1000#customer lifetime value"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "- TN=people we predicted not to churn who wont churn. We associate no cost with this as they continue being our customers\n", 
        "- FP=people we predict to churn. Who wont. Lets associate a `admin_cost+offer_cost` cost per customer with this as we will spend some money on getting them not to churn, but we will lose this money.\n", 
        "- FN=people we predict wont churn. And we send them nothing. But they will. This is the big loss, the `clv`\n", 
        "- TP= people who we predict will churn. And they will. These are the people we can do something with. So we make them an offer. Say a fraction f accept it. Our cost is\n", 
        "\n", 
        "`f * offer_cost + (1-f)*(clv+admin_cost)`\n", 
        "\n", 
        "This model can definitely be made more complex.\n", 
        "\n", 
        "Lets assume a conversion fraction of 0.5"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 58, 
      "cell_type": "code", 
      "source": [
        "conv=0.5\n", 
        "tnc = 0.\n", 
        "fpc = admin_cost+offer_cost\n", 
        "fnc = clv\n", 
        "tpc = conv*offer_cost + (1. - conv)*(clv+admin_cost)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 59, 
      "cell_type": "code", 
      "source": [
        "cost=np.array([[tnc,fpc],[fnc, tpc]])\n", 
        "cost"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We can compute the average cost(profit) per person using the following formula, which calculates the \"expected value\" of the per-customer loss/cost(profit):\n", 
        "\n", 
        "\\begin{eqnarray}\n", 
        "Cost &=& c(1P,1A) \\times p(1P,1A) + c(1P,0A) \\times p(1P,0A) + c(0P,1A) \\times p(0P,1A) + c(0P,0A) \\times p(0P,0A) \\\\\n", 
        "&=& \\frac{TP \\times c(1P,1A) + FP \\times c(1P,0A) + FN \\times c(0P,1A) + TN \\times c(0P,0A)}{N}\n", 
        "\\end{eqnarray}\n", 
        "\n", 
        "where N is the total size of the test set, 1P is predictions for class 1, or positives, 0A is actual values of the negative class in the test set. The first formula above just weighs the cost of a combination of observed and predicted with the out-of-sample probability of the combination occurring. The probabilities are \"estimated\" by the corresponding confusion matrix on the test set. (We'll provide a proof of this later in the course for the mathematically inclined, or just come bug Rahul at office hour if you cant wait!)\n", 
        "\n", 
        "The cost can thus be found by multiplying the cost matrix by the confusion matrix elementwise, and dividing by the sum of the elements in the confusion matrix, or the test set size.\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 60, 
      "cell_type": "code", 
      "source": [
        "def average_cost(y, ypred, cost):\n", 
        "    c=confusion_matrix(y,ypred)\n", 
        "    score=np.sum(c*cost)/np.sum(c)\n", 
        "    return score"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 61, 
      "cell_type": "code", 
      "source": [
        "def percentage(tpr, fpr, priorp, priorn):\n", 
        "    perc = tpr*priorp + fpr*priorn\n", 
        "    return perc\n", 
        "def av_cost2(tpr, fpr, cost, priorp, priorn):\n", 
        "    profit = priorp*(cost[1][1]*tpr+cost[1][0]*(1.-tpr))+priorn*(cost[0][0]*(1.-fpr) +cost[0][1]*fpr)\n", 
        "    return profit\n", 
        "def plot_cost(name, clf, ytest, xtest, cost, ax=None, threshold=False, labe=200, proba=True):\n", 
        "    initial=False\n", 
        "    if not ax:\n", 
        "        ax=plt.gca()\n", 
        "        initial=True\n", 
        "    if proba:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n", 
        "    else:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n", 
        "    priorp=np.mean(ytest)\n", 
        "    priorn=1. - priorp\n", 
        "    ben=[]\n", 
        "    percs=[]\n", 
        "    for i,t in enumerate(thresholds):\n", 
        "        perc=percentage(tpr[i], fpr[i], priorp, priorn)\n", 
        "        ev = av_cost2(tpr[i], fpr[i], cost, priorp, priorn)\n", 
        "        ben.append(ev)\n", 
        "        percs.append(perc*100)\n", 
        "    ax.plot(percs, ben, '-', alpha=0.3, markersize=5, label='cost curve for %s' % name)\n", 
        "    if threshold:\n", 
        "        label_kwargs = {}\n", 
        "        label_kwargs['bbox'] = dict(\n", 
        "        boxstyle='round,pad=0.3', alpha=0.2,\n", 
        "        )\n", 
        "        for k in range(0, fpr.shape[0],labe):\n", 
        "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n", 
        "            threshold = str(np.round(thresholds[k], 2))\n", 
        "            ax.annotate(threshold, (percs[k], ben[k]), **label_kwargs)\n", 
        "    ax.legend(loc=\"lower right\")\n", 
        "    return ax"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 62, 
      "cell_type": "code", 
      "source": [
        "ax = plot_cost(\"knn\",clfknn, ytest, Xtest, cost, threshold=True, labe=1);\n", 
        "plot_cost(\"logistic\",clfl, ytest, Xtest, cost, ax, threshold=True, labe=50);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### Things\n", 
        "\n", 
        "- standardizing separately and data snooping and pca\n", 
        "- train test on same data\n", 
        "- using a score in the cval function\n", 
        "- pipelines"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }
  ]
}